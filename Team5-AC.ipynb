{"cells":[{"cell_type":"markdown","source":["Krantas Konstantinos, 9975  \n","Strikos Konstantinos, 9517"],"metadata":{"id":"aJkCe9jqnJTw"}},{"cell_type":"code","execution_count":60,"metadata":{"id":"iI1yYDScs5pb","executionInfo":{"status":"ok","timestamp":1704475894051,"user_tz":-120,"elapsed":411,"user":{"displayName":"Konstantinos Strikos","userId":"00806009806564184535"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","df = pd.read_csv(\"dataset.csv\")\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","from mlxtend.plotting import plot_decision_regions\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.svm import SVC\n","\n","\n","# X = df.iloc[:, :2]\n","# y = df.iloc[:, 2]\n","X = df.iloc[:, :-1].values\n","y = df.iloc[:, -1].values\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)"]},{"cell_type":"markdown","metadata":{"id":"R5CceIZhEbbX"},"source":["# **1st Question - Bayes Classifier**"]},{"cell_type":"markdown","metadata":{"id":"HwyE090qD5t7"},"source":["# Bayes Classifier for Different Covariance matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bNNXASNKBNrV"},"outputs":[],"source":["class DiffCovMatBayesClassifier:\n","    def __init__(self):\n","        self.class_priors = {}\n","        self.class_means = {}\n","        self.class_covs = {}\n","\n","    def fit(self, X, y):\n","        classes = np.unique(y)\n","\n","        for label in classes:\n","            # Calculate the a priori probability of each class\n","            self.class_priors[label] = np.sum(y == label) / len(y)\n","\n","            # Calculate the mean of each class\n","            self.class_means[label] = np.mean(X[y == label], axis=0)\n","\n","            # Calculate the covariance matrix for each class\n","            self.class_covs[label] = np.cov(X[y == label], rowvar=False)\n","\n","    def predict(self, X):\n","        predictions = []\n","        for x in X:\n","            posteriors = []\n","            for label, prior in self.class_priors.items():\n","                # Calculate the likelihood of class' according to Bayes' theorem\n","                likelihood = self.multivariate_normal_pdf(x, self.class_means[label], self.class_covs[label])\n","                posterior = prior * likelihood\n","                posteriors.append(posterior)\n","\n","            # Choosing the class with the highest probability\n","            prediction = list(self.class_priors.keys())[np.argmax(posteriors)]\n","            predictions.append(prediction)\n","        return np.array(predictions)\n","\n","    def multivariate_normal_pdf(self, x, mean, cov):\n","        # Calculate the probability density for normal distribution\n","        constant = 1 / ((2 * np.pi) ** (len(x) / 2) * np.linalg.det(cov) ** 0.5)\n","        exponent = -0.5 * np.dot(np.dot((x - mean).T, np.linalg.inv(cov)), (x - mean))\n","        return constant * np.exp(exponent)\n","\n","# Create and train the Bayes classifier\n","bayes_classifier = DiffCovMatBayesClassifier()\n","bayes_classifier.fit(X_train, y_train)\n","\n","y_pred = bayes_classifier.predict(X_test)\n","\n","# Calculate the error\n","accuracy = np.mean(y_pred == y_test)\n","print(f\"error: {1 - accuracy}\")"]},{"cell_type":"markdown","metadata":{"id":"x259zUzWELdX"},"source":["# Visualisation for Different Covariance matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"af1E68PcQZun"},"outputs":[],"source":["# Classification Regions\n","\n","plot_decision_regions(X_test, y_pred, clf=bayes_classifier, legend=2)\n","plt.savefig(\"bayes_diffcovmat_regions.png\")\n","plt.show()\n","\n","# Plot test set points\n","plt.scatter(\n","    X_test[:,0],\n","    X_test[:,1],\n","    c=y_pred,\n","    cmap=plt.cm.Paired,\n","    marker='o',\n","    edgecolors='k',\n","    label='Test Set'\n",")\n","\n","# Find misclassified points\n","misclassified_points = X_test[y_pred != y_test]\n","\n","# Plot misclassified points\n","plt.scatter(\n","    misclassified_points[:,0],\n","    misclassified_points[:,1],\n","    s=100,\n","    facecolors='r',\n","    marker='x',\n","    label='Misclassified Points'\n",")\n","plt.title('Test set visualisation with misclassified points for Bayes with different covariance matrix')\n","plt.legend()\n","plt.savefig(\"bayes_diffcovmat.png\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"JA09TZlsDzRw"},"source":["# Bayes Classifier for Shared Covariance Matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Ewui2BwOdaI"},"outputs":[],"source":["class SharedCovMatBayesClassifier:\n","    def __init__(self):\n","        self.class_priors = {}\n","        self.class_means = {}\n","        self.shared_cov = None\n","\n","    def fit(self, X, y):\n","        classes = np.unique(y)\n","\n","        # Calculate the shared covariance matrix\n","        self.shared_cov = np.cov(X, rowvar=False)\n","\n","        for label in classes:\n","            # Calculate the class prior\n","            self.class_priors[label] = np.sum(y == label) / len(y)\n","\n","            # Calculate the class mean\n","            self.class_means[label] = np.mean(X[y == label], axis=0)\n","\n","    def predict(self, X):\n","        predictions = []\n","        for x in X:\n","            posteriors = []\n","            for label, prior in self.class_priors.items():\n","                # Calculate the likelihood using the shared covariance matrix\n","                likelihood = self.multivariate_normal_pdf(x, self.class_means[label], self.shared_cov)\n","                posterior = prior * likelihood\n","                posteriors.append(posterior)\n","\n","            # Select the class with the highest probability\n","            prediction = list(self.class_priors.keys())[np.argmax(posteriors)]\n","            predictions.append(prediction)\n","        return np.array(predictions)\n","\n","    def multivariate_normal_pdf(self, x, mean, cov):\n","        # Calculate the probability density function for a multivariate normal distribution\n","        constant = 1 / ((2 * np.pi) ** (len(x) / 2) * np.linalg.det(cov) ** 0.5)\n","        exponent = -0.5 * np.dot(np.dot((x - mean).T, np.linalg.inv(cov)), (x - mean))\n","        return constant * np.exp(exponent)\n","\n","# Create and train the Bayes classifier\n","bayes_classifier = SharedCovMatBayesClassifier()\n","bayes_classifier.fit(X_train, y_train)\n","\n","y_pred = bayes_classifier.predict(X_test)\n","\n","# Calculate the error\n","accuracy = np.mean(y_pred == y_test)\n","print(f\"Error: {1 - accuracy}\")"]},{"cell_type":"markdown","metadata":{"id":"V7QaEQBrfFYf"},"source":["# Visualisation for shared covariance matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WdkvXRDAR404"},"outputs":[],"source":["# Classification Regions\n","plot_decision_regions(X_test, y_pred, clf=bayes_classifier, legend=2)\n","plt.savefig(\"bayes_sharedcovmat_regions.png\")\n","plt.show()\n","\n","# Plot test set points\n","plt.scatter(\n","    X_test[:,0],\n","    X_test[:,1],\n","    c=y_pred,\n","    cmap=plt.cm.Paired,\n","    marker='o',\n","    edgecolors='k',\n","    label='Test Set'\n",")\n","\n","\n","# Find misclassified points\n","misclassified_points = X_test[y_pred != y_test]\n","\n","#Plot misclassified points\n","plt.scatter(\n","    misclassified_points[:,0],\n","    misclassified_points[:,1],\n","    s=100,\n","    facecolors='r',\n","    marker='x',\n","    label='Misclassified Points'\n",")\n","plt.title('Test set visualisation with misclassified points for Bayes with shared covariance matrix')\n","plt.legend()\n","plt.savefig(\"bayes_sharedcovmat.png\")\n","plt.show()"]},{"cell_type":"markdown","source":["Experimenting on both cases we conclude that the Bayes Classifier is more effective when the covariance matrix is not the shared for all classes, since the error is 0.17142857142857137 for different matrices and 0.26428571428571423 for shared. This is expected because when we force shared covariance matrix, if the features are not totally independent, this assumption may lead to inaccuracies because it is limiting the problem"],"metadata":{"id":"6jhSUKB3CJBg"}},{"cell_type":"markdown","metadata":{"id":"scuDkpDhFbig"},"source":["# **2nd Question - k-NN Classifier**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"46JKQyuLbtRu"},"outputs":[],"source":["error = []\n","for k in range(1,11):\n","  neigh = KNeighborsClassifier(k)\n","  neigh.fit(X_train, y_train)\n","  pred = neigh.predict(X_test)\n","  accuracy = accuracy_score(y_test, pred)\n","  current_error = 1-accuracy\n","  print (f\"The error for {k}-nn is: {current_error}\\n\")\n","  error.append(current_error)\n","  # Creating df for the labels\n","  class_1_df = X_test[pred == 1]\n","  class_2_df = X_test[pred == 2]\n","  class_3_df = X_test[pred == 3]\n","\n","  plt.figure()\n","  plot_decision_regions(X_test, pred, clf=neigh, legend=2)\n","\n","  plt.title(f'Decision Regions for k={k}', loc = 'left')\n","  plt.legend()\n","  plt.savefig(f'{k}-NN.png')\n","  plt.show()\n","\n","plt.figure()\n","plt.plot(range(1, 11), error, color='blue', linestyle=\"dotted\", marker=\"o\", markerfacecolor='red')\n","plt.ylabel('Error')\n","plt.title('Error for k=1 to 10')\n","plt.savefig(\"kNN_errors.png\")\n","plt.show()\n"]},{"cell_type":"markdown","source":["The error for k = 5, 6, 7 & 8 remains fixed, despite the fact that decision regions do not remain the same for each k.\n","We also see that the error for those k is lower than the error in both Bayes Classifiers. This seems normal, because:  \n","1.   We used the stock k-NN function, whereas before we created our own.\n","2.   Our dataset is also very small, so the difference between Bayes and k-NN on big data is not easily visible.\n"],"metadata":{"id":"nuhtPQH0tm8r"}},{"cell_type":"markdown","source":["# **3rd Question - SVM**"],"metadata":{"id":"008riK7Du-Or"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TrJ2vAgzTNAG"},"outputs":[],"source":["# Testing the values of C and gamma to find the optimal\n","Cs = [0.1,1,10]\n","gammas = [0.2,2,20]\n","\n","# Initialize an array to store error values\n","errors = np.zeros((len(gammas), len(Cs)))\n","\n","for i, gamma in enumerate(gammas):\n","    for j, C in enumerate(Cs):\n","        # Train an SVM with the RBF kernel, the current gamma, and C values\n","        clf = svm.SVC(kernel='rbf', gamma=gamma, C=C)\n","        clf.fit(X_train, y_train)\n","        y_pred = clf.predict(X_test)\n","        error = 1 - accuracy_score(y_test, y_pred)\n","        errors[i, j] = error\n","        print(f'The error for gamma={gamma} and C={C} is: {error}\\n')\n","\n","fig, ax1 = plt.subplots(figsize=(10, 6))\n","\n","# Plot error values\n","for j, C in enumerate(Cs):\n","    ax1.plot(gammas, errors[:, j], label=f'Error (C={C})', marker='x', linestyle='--')\n","\n","ax1.set_xlabel('Gamma')\n","ax1.set_ylabel('Error')\n","ax1.legend(loc='upper left')\n","ax2 = ax1.twinx()\n","ax2.set_ylabel('Error', color='red')\n","plt.title('Error for Different Gamma and C Values with RBF Kernel')\n","plt.show()\n","# It is visible that the best accuracy is for C = 1 & gamma = 0.2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FwxUCNF-GZVV"},"outputs":[],"source":["C_test = 1\n","gamma_test = 0.2\n","svm_model = SVC(kernel='rbf', C=C_test, gamma=gamma_test)\n","svm_model.fit(X_train, y_train)\n","predictions = svm_model.predict(X_test)\n","\n","# Plot decision regions\n","plt.figure()\n","plt.title(f'SVM Decision Regions for C={C_test}, gamma={gamma_test}')\n","plot_decision_regions(X, y, clf=svm_model, legend=2)\n","plt.savefig(\"svmregions.png\")\n","plt.show()\n","\n","# Plot train and test set\n","plt.figure()\n","plt.scatter(\n","    X_train[:,0],\n","    X_train[:,1],\n","    c=y_train,\n","    cmap=plt.cm.Paired,\n","    marker='o',\n","    edgecolors='k',\n","    label='Train Set'\n",")\n","plt.scatter(\n","    X_test[:,0],\n","    X_test[:,1],\n","    c=predictions,\n","    cmap=plt.cm.Paired,\n","    marker='^',\n","    edgecolors='k',\n","    label='Test Set'\n",")\n","plt.legend()\n","plt.title(\"Train and test set\")\n","plt.savefig(\"SVM_train_test.png\")\n","plt.show()\n","\n","# Plot support vectors and train set\n","plt.figure()\n","plt.scatter(\n","    X_train[:,0],\n","    X_train[:,1],\n","    c=y_train,\n","    cmap=plt.cm.Paired,\n","    marker='o',\n","    edgecolors='k',\n","    label='Train Set'\n",")\n","plt.scatter(\n","    svm_model.support_vectors_[:, 0],\n","    svm_model.support_vectors_[:, 1],\n","    s=80,\n","    facecolors='k',\n","    marker='x',\n","    label='Support Vectors'\n",")\n","plt.legend()\n","plt.title(\"Support vectors and train set\")\n","plt.savefig(\"SVM_train_SV.png\")\n","plt.show()\n","\n","# Plot test set with misclassified points\n","\n","# Find misclassified points\n","misclassified_points = X_test[predictions != y_test]\n","\n","plt.scatter(\n","    X_test[:,0],\n","    X_test[:,1],\n","    c=predictions,\n","    cmap=plt.cm.Paired,\n","    marker='o',\n","    edgecolors='k',\n","    label='Test Set'\n",")\n","#Plot misclassified points\n","plt.scatter(\n","    misclassified_points[:,0],\n","    misclassified_points[:,1],\n","    s=80,\n","    facecolors='r',\n","    marker='x',\n","    label='Misclassified Points'\n",")\n","plt.legend()\n","plt.savefig(\"SVM_test_misclassified.png\")\n","plt.show()"]},{"cell_type":"markdown","source":["There is a variety of results when we modify the hyperparameters C and gamma, whose optimal combination depends on each dataset. At our example, we experimented with different values and concluded that C=1 and gamma=0.2 are the best choices, in order to achieve the highest accuracy possible.  \n","Comparing with previous classifiers, it is safe to say that a SVM Classifier, properly modified is way better than a Bayes Classifier. When it comes to comparing with k-NN it is not so clear, since the difference of their accuracies is not very high, so in our case it seems that they are very close and SVM is slightly less accurate."],"metadata":{"id":"gBm0NH1KOpLq"}}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}